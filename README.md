# GPT From Scratch

## Day 1
- Implemented character-level tokenizer
- Built vocabulary from dataset
- Created encode/decode functions
- Generated input-target training pairs

Dataset: Tiny Shakespeare
Vocabulary size:65